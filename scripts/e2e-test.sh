#!/bin/bash

# E2E Test Suite for Take-Home Backend
# ====================================
# This script performs comprehensive end-to-end testing of the import pipeline:
# 1. Validates environment and dependencies
# 2. Checks Supabase and API health
# 3. Tests JSON upload with valid data
# 4. Tests multipart file upload
# 5. Tests stream upload
# 6. Tests validation with invalid data
# 7. Runs worker to process import jobs
# 8. Verifies worker success and contact processing
# 9. Tests error handling and 404 responses
#
# Prerequisites:
# - Supabase running locally (pnpm run supabase:start)
# - API server running (pnpm run dev)
# - jq and curl installed
# - test data files present

set -euo pipefail  # Exit on error, undefined vars, pipe failures

# =============================================================================
# CONFIGURATION
# =============================================================================

API_URL="http://localhost:3020"
TEST_DATA_FILE="scripts/test-data.json"
HEALTH_ENDPOINT="$API_URL/health"
IMPORT_ENDPOINT="$API_URL/import"

# =============================================================================
# UTILITY FUNCTIONS
# =============================================================================

print_header() {
    echo ""
    echo "=== $1 ==="
    echo ""
}

print_result() {
    local exit_code=$1
    local message=$2
    local details=${3:-}
    
    if [ $exit_code -eq 0 ]; then
        echo "✅ $message"
    else
        echo "❌ $message"
        if [ -n "$details" ]; then
            echo "   Details: $details"
        fi
        exit 1
    fi
}

wait_for_service() {
    local url=$1
    local max_attempts=30
    local attempt=1
    
    echo "Waiting for service at $url to be ready..."
    
    while [ $attempt -le $max_attempts ]; do
        if curl -s "$url" > /dev/null 2>&1; then
            return 0
        fi
        echo "   Attempt $attempt/$max_attempts - service not ready yet..."
        sleep 2
        attempt=$((attempt + 1))
    done
    
    return 1
}

extract_json_value() {
    local json=$1
    local key=$2
    echo "$json" | grep -o "\"$key\":\"[^\"]*\"" | cut -d'"' -f4
}

verify_worker_success() {
    local worker_output=$1
    if echo "$worker_output" | grep -q "Successfully processed.*contacts"; then
        return 0
    elif echo "$worker_output" | grep -q "Worker completed successfully\|Worker.*stopping"; then
        return 0
    else
        return 1
    fi
}

make_api_request() {
    local method=$1
    local endpoint=$2
    local data=${3:-}
    local headers=${4:-"Content-Type: application/json"}
    
    local curl_args=("-s" "-X" "$method" "$endpoint")
    
    if [ -n "$headers" ]; then
        curl_args+=("-H" "$headers")
    fi
    
    if [ -n "$data" ]; then
        if [ -f "$data" ]; then
            curl_args+=("-d" "@$data")
        else
            curl_args+=("-d" "$data")
        fi
    fi
    
    curl "${curl_args[@]}"
}

test_validation_error() {
    local test_name=$1
    local test_data=$2
    local expected_pattern=$3
    
    echo "Testing $test_name..."
    local response=$(make_api_request "POST" "$IMPORT_ENDPOINT" "$test_data")
    
    if echo "$response" | grep -q "$expected_pattern"; then
        print_result 0 "$test_name properly rejected"
        echo "   Response: $response"
    else
        print_result 1 "$test_name was not properly rejected" "$response"
    fi
}

# =============================================================================
# VALIDATION TESTS
# =============================================================================

validate_dependencies() {
    print_header "DEPENDENCY VALIDATION"
    
    echo "Checking required dependencies..."
    
    # Check curl
    if ! command -v curl &> /dev/null; then
        print_result 1 "curl is not installed" "Please install curl"
    fi
    print_result 0 "curl is available"
    
    # Check jq
    if ! command -v jq &> /dev/null; then
        print_result 1 "jq is not installed" "Please install jq: brew install jq (macOS) or apt-get install jq (Ubuntu)"
    fi
    print_result 0 "jq is available"
    
    # Check test data files
    echo "Checking test data file..."
    if [ ! -f "$TEST_DATA_FILE" ]; then
        print_result 1 "Test data file not found" "Expected: $TEST_DATA_FILE"
    fi
    print_result 0 "Test data file exists"
}

validate_services() {
    print_header "SERVICE VALIDATION"
    
    # Check Supabase
    echo "Checking Supabase status..."
    local supabase_status=$(pnpm supabase status 2>&1)
    if echo "$supabase_status" | grep -q "running"; then
        print_result 0 "Supabase is running"
    else
        print_result 1 "Supabase is not running" "Run 'pnpm run supabase:start' first"
    fi
    
    # Check API
    if ! wait_for_service "$API_URL"; then
        print_result 1 "API is not responding" "Run 'pnpm run dev' first"
    fi
    print_result 0 "API is responding"
    
    # Test health endpoint
    echo "Testing health endpoint..."
    local health_response=$(make_api_request "GET" "$HEALTH_ENDPOINT")
    if echo "$health_response" | grep -q '"status":"healthy"'; then
        print_result 0 "Health endpoint reports healthy status"
    else
        print_result 1 "Health endpoint reports unhealthy status" "$health_response"
    fi
}

# =============================================================================
# JSON UPLOAD TESTS
# =============================================================================

test_json_upload() {
    print_header "JSON UPLOAD TESTS"
    
    # Count expected contacts from large test data
    echo "Counting expected contacts from large test data..."
    local expected_count=$(jq '.data | length' "$TEST_DATA_FILE")
    echo "   Expected contacts: $expected_count"
    
    # Test valid JSON upload with large dataset (non-resumable)
    echo "Testing JSON upload endpoint with large dataset (non-resumable)..."
    local json_response=$(make_api_request "POST" "$IMPORT_ENDPOINT" "$TEST_DATA_FILE")
    
    if echo "$json_response" | grep -q "jobId"; then
        local job_id=$(extract_json_value "$json_response" "jobId")
        print_result 0 "JSON upload accepted with jobId: $job_id"
        echo "   Response: $json_response"
        
        # Store job ID for worker processing
        echo "$job_id" > /tmp/json_upload_job_id
    else
        print_result 1 "JSON upload endpoint failed" "$json_response"
    fi
    
    # Test valid JSON upload with large dataset (resumable)
    echo "Testing JSON upload endpoint with large dataset (resumable)..."
    local json_resumable_response=$(curl -s -X POST "$IMPORT_ENDPOINT" \
        -H "Content-Type: application/json" \
        -d "{\"source\": \"e2e-test-resumable\", \"data\": $(jq '.data' "$TEST_DATA_FILE"), \"useResumable\": true}")
    
    if echo "$json_resumable_response" | grep -q "jobId"; then
        local job_id=$(extract_json_value "$json_resumable_response" "jobId")
        print_result 0 "JSON resumable upload accepted with jobId: $job_id"
        echo "   Response: $json_resumable_response"
        
        # Store job ID for worker processing
        echo "$job_id" >> /tmp/json_upload_job_id
    else
        print_result 1 "JSON resumable upload endpoint failed" "$json_resumable_response"
    fi
}

# =============================================================================
# MULTIPART UPLOAD TESTS
# =============================================================================

test_multipart_upload() {
    print_header "MULTIPART UPLOAD TESTS"
    
    # Create a temporary file for multipart upload
    local temp_file="/tmp/test_contacts_large.json"
    cp "$TEST_DATA_FILE" "$temp_file"
    
    # Test multipart file upload with large dataset (non-resumable)
    echo "Testing multipart file upload with large dataset (non-resumable)..."
    local multipart_response=$(curl -s -X POST "$IMPORT_ENDPOINT" \
        -F "file=@$temp_file" \
        -F "source=e2e-test-multipart-large" \
        -F "useResumable=false")
    
    if echo "$multipart_response" | grep -q "jobId"; then
        local job_id=$(extract_json_value "$multipart_response" "jobId")
        print_result 0 "Multipart upload accepted with jobId: $job_id"
        echo "   Response: $multipart_response"
        
        # Store job ID for worker processing
        echo "$job_id" >> /tmp/multipart_upload_job_ids
    else
        print_result 1 "Multipart upload endpoint failed" "$multipart_response"
    fi
    
    # Test multipart file upload with large dataset (resumable)
    echo "Testing multipart file upload with large dataset (resumable)..."
    local multipart_resumable_response=$(curl -s -X POST "$IMPORT_ENDPOINT" \
        -F "file=@$temp_file" \
        -F "source=e2e-test-multipart-resumable" \
        -F "useResumable=true")
    
    if echo "$multipart_resumable_response" | grep -q "jobId"; then
        local job_id=$(extract_json_value "$multipart_resumable_response" "jobId")
        print_result 0 "Multipart resumable upload accepted with jobId: $job_id"
        echo "   Response: $multipart_resumable_response"
        
        # Store job ID for worker processing
        echo "$job_id" >> /tmp/multipart_upload_job_ids
    else
        print_result 1 "Multipart resumable upload endpoint failed" "$multipart_resumable_response"
    fi
    
    # Clean up temp file
    rm -f "$temp_file"
}

# =============================================================================
# STREAM UPLOAD TESTS
# =============================================================================

test_stream_upload() {
    print_header "STREAM UPLOAD TESTS"
    
    # Test stream upload with large dataset (non-resumable)
    echo "Testing stream upload with large dataset (non-resumable)..."
    local stream_response=$(curl -s -X POST "$IMPORT_ENDPOINT" \
        -H "Content-Type: application/octet-stream" \
        -H "X-Source: e2e-test-stream-large" \
        -H "X-Use-Resumable: false" \
        --data-binary "@$TEST_DATA_FILE")
    
    if echo "$stream_response" | grep -q "jobId"; then
        local job_id=$(extract_json_value "$stream_response" "jobId")
        print_result 0 "Stream upload accepted with jobId: $job_id"
        echo "   Response: $stream_response"
        
        # Store job ID for worker processing
        echo "$job_id" >> /tmp/stream_upload_job_ids
    else
        print_result 1 "Stream upload endpoint failed" "$stream_response"
    fi
    
    # Test stream upload with large dataset (resumable)
    echo "Testing stream upload with large dataset (resumable)..."
    local stream_resumable_response=$(curl -s -X POST "$IMPORT_ENDPOINT" \
        -H "Content-Type: application/octet-stream" \
        -H "X-Source: e2e-test-stream-resumable" \
        -H "X-Use-Resumable: true" \
        --data-binary "@$TEST_DATA_FILE")
    
    if echo "$stream_resumable_response" | grep -q "jobId"; then
        local job_id=$(extract_json_value "$stream_resumable_response" "jobId")
        print_result 0 "Stream resumable upload accepted with jobId: $job_id"
        echo "   Response: $stream_resumable_response"
        
        # Store job ID for worker processing
        echo "$job_id" >> /tmp/stream_upload_job_ids
    else
        print_result 1 "Stream resumable upload endpoint failed" "$stream_resumable_response"
    fi
}

# =============================================================================
# WORKER PROCESSING TESTS
# =============================================================================

test_worker_processing() {
    print_header "WORKER PROCESSING TESTS"
    
    # Count total expected contacts from all uploads (6 uploads: 3 types × 2 modes each)
    local total_expected=$(jq '.data | length' "$TEST_DATA_FILE")
    local num_uploads=6  # JSON (non-resumable + resumable), Multipart (non-resumable + resumable), Stream (non-resumable + resumable)
    local expected_contacts=$((total_expected * num_uploads))
    
    echo "Expected contacts from current test: $expected_contacts (${total_expected} per upload × ${num_uploads} uploads)"
    echo "Note: Worker may process additional jobs from previous test runs"
    echo "Starting worker processing..."
    
    # Run worker to process all jobs
    echo "Running worker to process all import jobs..."
    local start_time=$(date +%s)
    local worker_output=$(pnpm run worker:run-once 2>&1)
    local worker_exit_code=$?
    local end_time=$(date +%s)
    local processing_time=$((end_time - start_time))
    
    if [ $worker_exit_code -eq 0 ]; then
        print_result 0 "Worker completed successfully in ${processing_time} seconds"
        echo "   Worker output: $worker_output"
    else
        print_result 1 "Worker failed" "$worker_output"
    fi
    
    # Verify worker success
    echo "Verifying worker processed contacts successfully..."
    if verify_worker_success "$worker_output"; then
        print_result 0 "Worker successfully processed contacts"
        
        # Extract all processed counts and sum them up
        local processed_counts=$(echo "$worker_output" | grep -o "Successfully processed [0-9]* contacts" | grep -o "[0-9]*")
        local total_processed=0
        local num_jobs_processed=0
        
        if [ -n "$processed_counts" ]; then
            # Sum up all the processed counts
            while IFS= read -r count; do
                if [ -n "$count" ] && [ "$count" -gt 0 ]; then
                    total_processed=$((total_processed + count))
                    num_jobs_processed=$((num_jobs_processed + 1))
                fi
            done <<< "$processed_counts"
            
            echo "   Individual job counts: $processed_counts"
            echo "   Number of jobs processed: $num_jobs_processed"
            echo "   Total processed contacts: $total_processed"
            echo "   Expected contacts from current test: $expected_contacts"
            echo "   Processing time: ${processing_time} seconds"
            
            # Check if we processed at least the expected number of contacts
            if [ "$total_processed" -ge "$expected_contacts" ]; then
                print_result 0 "All expected contacts from current test were processed correctly"
                if [ "$total_processed" -gt "$expected_contacts" ]; then
                    echo "   Note: Additional contacts processed from previous test runs"
                fi
            else
                print_result 1 "Contact count mismatch" "Expected at least $expected_contacts, processed $total_processed"
            fi
        fi
        
        # Check for any errors in processing
        if echo "$worker_output" | grep -q "Failed to process\|error"; then
            print_result 1 "Worker encountered errors during processing" "$worker_output"
        else
            print_result 0 "No errors detected during worker processing"
        fi
    else
        print_result 1 "Worker failed to process contacts" "$worker_output"
    fi
}

# =============================================================================
# VALIDATION TESTS
# =============================================================================

test_data_validation() {
    print_header "DATA VALIDATION TESTS"
    
    # Test invalid JSON data (missing email)
    test_validation_error \
        "invalid JSON data (missing email)" \
        '{"source": "test", "data": [{"name": "John"}]}' \
        "error\|issues"
    
    # Test empty data array
    test_validation_error \
        "empty data array" \
        '{"source": "test", "data": []}' \
        "error\|issues"
    
    # Test missing required fields
    test_validation_error \
        "missing required fields" \
        '{"source": "test", "data": [{"name": "John"}]}' \
        "error\|issues"
    
    # Test invalid email format
    test_validation_error \
        "invalid email format" \
        '{"source": "test", "data": [{"name": "John", "email": "invalid-email"}]}' \
        "error\|issues"
    
    # Test invalid multipart upload (missing file)
    echo "Testing invalid multipart upload (missing file)..."
    local invalid_multipart_response=$(curl -s -X POST "$IMPORT_ENDPOINT" \
        -F "source=e2e-test-invalid" \
        -F "useResumable=false")
    
    if echo "$invalid_multipart_response" | grep -q "error\|Missing required field"; then
        print_result 0 "Invalid multipart upload properly rejected"
        echo "   Response: $invalid_multipart_response"
    else
        print_result 1 "Invalid multipart upload was not properly rejected" "$invalid_multipart_response"
    fi
    
    # Test invalid stream upload (missing headers)
    echo "Testing invalid stream upload (missing headers)..."
    local invalid_stream_response=$(curl -s -X POST "$IMPORT_ENDPOINT" \
        -H "Content-Type: application/octet-stream" \
        --data-binary "@$TEST_DATA_FILE")
    
    if echo "$invalid_stream_response" | grep -q "error\|Missing required parameters"; then
        print_result 0 "Invalid stream upload properly rejected"
        echo "   Response: $invalid_stream_response"
    else
        print_result 1 "Invalid stream upload was not properly rejected" "$invalid_stream_response"
    fi
    
    # Test large file upload performance validation
    echo "Testing large file upload performance validation..."
    local performance_response=$(curl -s -X POST "$IMPORT_ENDPOINT" \
        -F "file=@$TEST_DATA_FILE" \
        -F "source=e2e-test-performance" \
        -F "useResumable=false")
    
    if echo "$performance_response" | grep -q "jobId"; then
        print_result 0 "Performance test upload accepted"
        echo "   Response: $performance_response"
    else
        print_result 1 "Performance test upload failed" "$performance_response"
    fi
    
    # Test resumable upload validation
    echo "Testing resumable upload validation..."
    local resumable_validation_response=$(curl -s -X POST "$IMPORT_ENDPOINT" \
        -F "file=@$TEST_DATA_FILE" \
        -F "source=e2e-test-resumable-validation" \
        -F "useResumable=true")
    
    if echo "$resumable_validation_response" | grep -q "jobId"; then
        print_result 0 "Resumable validation test upload accepted"
        echo "   Response: $resumable_validation_response"
    else
        print_result 1 "Resumable validation test upload failed" "$resumable_validation_response"
    fi
}

# =============================================================================
# ERROR HANDLING TESTS
# =============================================================================

test_error_handling() {
    print_header "ERROR HANDLING TESTS"
    
    # Test health endpoint after imports
    echo "Testing health endpoint after imports..."
    local post_import_health=$(make_api_request "GET" "$HEALTH_ENDPOINT")
    if echo "$post_import_health" | grep -q '"status":"healthy"'; then
        print_result 0 "System remains healthy after imports"
    else
        print_result 1 "System unhealthy after imports" "$post_import_health"
    fi
    
    # Test 404 endpoint
    echo "Testing 404 endpoint..."
    local not_found_response=$(make_api_request "GET" "$API_URL/nonexistent")
    if echo "$not_found_response" | grep -q '"error":"Not found"'; then
        print_result 0 "404 endpoint properly handled"
    else
        print_result 1 "404 endpoint not properly handled" "$not_found_response"
    fi
    
    # Test unsupported content type
    echo "Testing unsupported content type..."
    local unsupported_response=$(curl -s -X POST "$IMPORT_ENDPOINT" \
        -H "Content-Type: text/plain" \
        -d "plain text data")
    
    if echo "$unsupported_response" | grep -q "error\|Import failed"; then
        print_result 0 "Unsupported content type properly handled"
        echo "   Response: $unsupported_response"
    else
        print_result 1 "Unsupported content type not properly handled" "$unsupported_response"
    fi
}

# =============================================================================
# CLEANUP
# =============================================================================

cleanup() {
    echo "Cleaning up temporary files..."
    rm -f /tmp/json_upload_job_id
    rm -f /tmp/multipart_upload_job_ids
    rm -f /tmp/stream_upload_job_ids
    print_result 0 "Cleanup completed"
}

# =============================================================================
# MAIN EXECUTION
# =============================================================================

main() {
    echo "Starting Enhanced E2E Test Suite with Large Dataset and Resumable Upload Testing"
    echo "=============================================================================="
    
    validate_dependencies
    validate_services
    test_json_upload
    test_multipart_upload
    test_stream_upload
    test_worker_processing
    test_data_validation
    test_error_handling
    cleanup
    
    echo ""
    echo "=========================================="
    echo "🎉 Enhanced E2E Test Suite Completed Successfully! 🎉"
    echo "=========================================="
    echo ""
    echo "✅ Large Dataset and Resumable Upload Testing Results:"
    echo "   • JSON Upload: Working with large dataset (resumable + non-resumable)"
    echo "   • Multipart Upload: Working with large dataset (resumable + non-resumable)"
    echo "   • Stream Upload: Working with large dataset (resumable + non-resumable)"
    echo ""
    echo "✅ All Systems Operational:"
    echo "   • Supabase: Running"
    echo "   • API: Responding"
    echo "   • Health: Healthy"
    echo "   • Validation: Working"
    echo "   • Worker: Processing large datasets (resumable + non-resumable)"
    echo "   • Database: Updated with large datasets"
    echo "   • Error Handling: Working"
    echo "   • Resumable Uploads: Working"
    echo ""
    echo "🚀 All upload types handle large datasets with resumable functionality correctly!"
    echo ""
}

# Set up trap to ensure cleanup runs on exit
trap cleanup EXIT

main "$@"

